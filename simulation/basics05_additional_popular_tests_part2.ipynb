{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtBmLyF194WmgFjD7exolH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Experimentation Basics: $t$-tests"
      ],
      "metadata": {
        "id": "zUFpmsCExHm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will follow [this](https://en.wikipedia.org/wiki/Student%27s_t-test) pretty closely as it has one of the best explanations to the $t$-test. The goal is to simplify it even further and tie it back to A/B tests."
      ],
      "metadata": {
        "id": "fR1ROe6p0HdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $t$-test Introduction"
      ],
      "metadata": {
        "id": "ZWH8wkB-xHpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A $t$-test is any hypothesis test where the test statistic follows a Student's $t$-distribution under the null.\n",
        "\n",
        "It is often used when the test statistic *would* follow a normal distribution if the standard deviation (population) was known but is not - therefore must be estimated based on the observed data, when this occurs, under certain conditions the data is proven to follow a Student's $t$-distribution. Thus, the $Z$-test often yields similar results to the $t$-test, and converges as the sample size increases.\n",
        "\n",
        "Questions such as \"why or when would we ever know if our data fits that criteria\" will be answered below!"
      ],
      "metadata": {
        "id": "whPqNlbExHr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student's $t$-distribution"
      ],
      "metadata": {
        "id": "NtBR6M6qxHuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The $t$-distribution is a continuous probability distribution that resembles the normal distribution, except that it is typically a little fatter and lower (i.e., heavier tails). The $t$-distribution, also referred to as $t_\\nu$, has a parameter $\\nu$ that controls this width. For example, $\\nu=1$ means that $t_\\nu$ becomes what's known as a standard *Cauchy distribution*, which has fat tails, so fat in that it has an undefined mean despite being symmetric and \"bell\"-shaped. When $\\nu\\rightarrow\\infty$, it becomes the standard normal distribution ($\\mathcal{N}(0,1)$). Therefore, the higher the $\\nu$, the \"thinner\" the distribution gets. Due to this parameter, the $t$-distribution is referred to as a generalization of the standard normal distribution.\n",
        "\n",
        "The $t$-distribution has the following PDF:\n",
        "\n",
        "$$f(t)=\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\pi\\nu}\\Gamma(\\frac{\\nu}{2})}\\left(1+\\frac{t^2}{\\nu}\\right)^{-(\\nu+1)/2}$$\n",
        "\n",
        "where the $\\nu$ parameter is the number of *degrees of freedom* and $\\Gamma$ is the gamma function, defined as:\n",
        "\n",
        "$$\\Gamma(n)=(n-1)!$$\n"
      ],
      "metadata": {
        "id": "xKVEtgbh0aTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The uses of the $t$-test"
      ],
      "metadata": {
        "id": "wFYvkP_Q0aVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$t$-tests are used for one-sample and two-sample tests, similar to the $Z$-test's usecases.\n",
        "\n",
        "- A **one-sample** $t$-test: tests whether the mean of a population has a value specified in a null hypothesis\n",
        "- A **two-sample** $t$-test: tests if the means of two populations are equal. When the variance of the two populations are assumed to be equal, it is called a \"Student's $t$-test\", but when this assumptions is dropped it is called \"Welch's $t$-test\". These tests are often referred to as **unpaired* or *independent samples* $t$-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping, such as in a stereotypical A/B test.\n"
      ],
      "metadata": {
        "id": "QdAzb01_nt_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions of the $t$-test"
      ],
      "metadata": {
        "id": "XdoPzBASnuEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the previous section mentioned when we use this test, there should still be some leftover questions such as, \"these sound similar to a $Z$-test, when should we use one over the other?\", or even \"When would these assumptions even ever be fulfilled?\". This section should demystify some of these questions.\n",
        "\n",
        "Speaking generally, most test statistics actually have the form $t=Z/s$, where $Z$ and $s$ are both functions of the data. $s$ acts as a scaling parameter that allows the distribution of $t$ to be determined.\n",
        "\n",
        "As an example, in a one-sample $t$-test,\n",
        "\n",
        "$$t=\\frac{Z}{s}=\\frac{\\bar X-\\mu}{\\hat\\sigma/\\sqrt{n}}$$\n",
        "\n",
        "This should look similar to the $Z$ statistic, except that instead of a known population standard deviation $\\sigma$, we instead use $\\hat\\sigma$, which is an *estimate* of the standard deviation of the population, because one differentiating assumption with the $t$-test is that we assume to not know what the population standard deviation is.\n",
        "\n",
        "The assumptions for the one-sample $t$-test are:\n",
        "- $\\bar X$ follows a normal distribution $\\mathcal{N}(\\mu, \\sigma^2/n)$\n",
        "- $s^2(n-1)\\sigma^2$ follows a $\\chi^2$ distribution with $n-1$ degrees of freedom. This assumption is met when the observations used for estimating $s^2$ come from a normal distribution (and i.i.d. for each group).\n",
        "- $Z$ and $s$ are independent.\n",
        "\n",
        "For a two-sample $t$-test,\n",
        "- the means of the two populations should both follow normal distributions. As discussed previously, this often follows in large samples due to the central limit theorem even if the population being sampled is not normal.\n",
        "- The two populations being compared are assumed to have the same variance if using the original Student's $t$-test, although there are variations of this test that relax that assumption (Welch's $t$-test).\n",
        "- The data is sampled independently from the two populations being compared or be fully paired.\n",
        "\n",
        "From the above, we can see that given a stereotypical A/B test, the assumptions above should generally hold, especially the normality assumption, as typical A/B tests have higher sample sizes than how these methods were traditionally derived. Regarding the assumption that variance following $\\chi^2$, the *Slutsky's theorem* implies that the distribution of the sample variance has little effect on the distribution of the test statistic."
      ],
      "metadata": {
        "id": "wwkFhl7OpC_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paired vs Unpaired two-sample $t$-tests"
      ],
      "metadata": {
        "id": "OKycG5zY0aXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stereotypical A/B test may be an \"unpaired $t$-test\". *Paired* $t$-tests are special designs that lead to higher power than unpaired tests because of less noise. A paired test is when the two datasets (control and treatment) have an obvious and meaningful one-to-one correspondence. For example, if the two datasets we are comparing are a \"before\" and \"after\", then each person is compared to themselves. One has to be careful in which tests to use in different paired test designs though."
      ],
      "metadata": {
        "id": "BbzlOKwf1bDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of the above"
      ],
      "metadata": {
        "id": "jnhB7cO91bFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize the above, a typical A/B test will be a unpaired two-sample $t$-test or a two-sample $Z$-test. The threshold for which to use matters less and less as the sample size grows, and is often not a consideration in A/B tests where samples are in the thousands."
      ],
      "metadata": {
        "id": "BhojdJgW2fK6"
      }
    }
  ]
}
