{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfqTr+SOrEux7LMFhgUEnD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Testing"
      ],
      "metadata": {
        "id": "6XonWOxdBMKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Peeking' is a dangerous source of inflated false positives in an A/B test. However, the desire to 'know results early'\n",
        "is not always a bad one -\n",
        "for example, when a significant loss may be incurred by delaying an experiment decision,\n",
        "such as launching a new feature ahead of a major event ([More Examples Here](https://docs.statsig.com/experiments-plus/sequential-testing)).\n",
        "Sequential Testing is the body of work that develops and recommends\n",
        "methods in which A/B Test practicioners can, under controlled settings, 'peak' and make earlier decisions as necessary.\n",
        "\n",
        "A nice review of the current state of the literature of Sequential Stopping can be found at\n",
        "([Larsen et al., 2023](https://arxiv.org/pdf/2212.11366.pdf)). There is also a nice high level overview by Spotify\n",
        "([Schultzberg and Ankargren, 2023](https://engineering.atspotify.com/2023/03/choosing-sequential-testing-framework-comparisons-and-discussions/)).\n",
        "Note that the Spotify article's comment on StatSig's implementation is outdated - like many other companies, StatSig has switched to the mSPRT\n",
        "methodology described in ([Zhao et al., 2019](https://arxiv.org/pdf/1905.10493.pdf)).\n",
        "More on StatSig's implementation at ([Stewart, 2023](https://www.statsig.com/blog/sequential-testing-on-statsig))\n",
        "\n",
        "In this Simulation, we will code out sequential testing and compare it with a peeking test in the following areas:\n",
        "1. FPR (False Positive Rate)\n",
        "2. Power\n",
        "\n",
        "for 3 different kinds of tests:\n",
        "1. fixed horizon test\n",
        "2. peeking test\n",
        "3. sequential test (mSPRT)\n",
        "\n",
        "These are heavily inspired by ([Stewart, 2023](https://www.statsig.com/blog/sequential-testing-on-statsig)) and ([Li, 2022](https://towardsdatascience.com/wish-tackles-peeking-with-always-valid-p-values-8a0782ac9654))"
      ],
      "metadata": {
        "id": "giM06qNdBMM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import uuid\n",
        "from scipy.stats import ttest_ind\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "rCMnTXLxD4i5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "KUbaVtY3CXoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate data\n",
        "def sample_data(traffic_per_day=100, days=14, param=0.037):\n",
        "  \"\"\"\n",
        "  generate A/B test data\n",
        "  \"\"\"\n",
        "  # Generate Poisson\n",
        "  samples = np.random.binomial(n=1, p=param, size=(traffic_per_day * days))\n",
        "\n",
        "  # Generate UUIDs\n",
        "  uuids = [str(uuid.uuid4()) for _ in range(days * traffic_per_day)]\n",
        "\n",
        "  # Generate evenly spaced days\n",
        "  days_column = np.repeat(np.arange(1, days + 1), traffic_per_day)\n",
        "\n",
        "  # Combine UUIDs, days, and numpy array\n",
        "  df = pd.DataFrame({\n",
        "      'userid': uuids,\n",
        "      'day': days_column,\n",
        "      'action_count': samples\n",
        "  })\n",
        "\n",
        "\n",
        "  return df\n",
        "\n",
        "# generate standard control and treatment\n",
        "def test_data(relative_lift=0.1, traffic_per_day=100, days=14, param=0.037):\n",
        "  \"\"\"\n",
        "  Control and Treatment\n",
        "  \"\"\"\n",
        "  return sample_data(traffic_per_day, days, param), sample_data(traffic_per_day, days, param*(1+relative_lift))"
      ],
      "metadata": {
        "id": "PbZO1WqrCW-z"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Fixed Horizon Test"
      ],
      "metadata": {
        "id": "wS6YsuQsMs6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_horizon_test(control, treatment, alpha=0.05):\n",
        "  \"\"\"\n",
        "  performs ttest once using full data\n",
        "  \"\"\"\n",
        "  t_statistic, p_value = ttest_ind(treatment['action_count'], control['action_count'], equal_var=False)\n",
        "\n",
        "  return 1 if p_value < alpha else 0"
      ],
      "metadata": {
        "id": "Ohq8ikcTMvvl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Peeking Test"
      ],
      "metadata": {
        "id": "YK9B2x44ODIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def peeking_test(control, treatment, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a peeking test using Welch's t-test.\n",
        "    \"\"\"\n",
        "    # Iterate over each day\n",
        "    for day in range(1, max(control['day']) + 1):\n",
        "        # Filter data up to the current day\n",
        "        control_data = control[control['day'] <= day]['action_count']\n",
        "        treatment_data = treatment[treatment['day'] <= day]['action_count']\n",
        "\n",
        "        # Perform Welch's t-test\n",
        "        t_statistic, p_value = ttest_ind(control_data, treatment_data, equal_var=False)\n",
        "\n",
        "        # Check if p-value is significant\n",
        "        if p_value < alpha:\n",
        "            return 1  # Significant result found, return 1\n",
        "\n",
        "    # No significant result found after testing all days\n",
        "    return 0"
      ],
      "metadata": {
        "id": "gR-Wy7zcOCfA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Test (mSPRT)"
      ],
      "metadata": {
        "id": "FNOjhcSgSNh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def msprt_pval(control, treatment, alpha = 0.05):\n",
        "  \"\"\"\n",
        "  performs msprt\n",
        "  \"\"\"\n",
        "  # Iterate over each day\n",
        "  for day in range(1, max(control['day']) + 1):\n",
        "      # Filter data up to the current day\n",
        "      x_c = control[control['day'] <= day]['action_count']\n",
        "      x_t = treatment[treatment['day'] <= day]['action_count']\n",
        "\n",
        "      # variance of the difference in means\n",
        "      v = (np.var(x_c) / len(x_c)) + (np.var(x_t) / len(x_t))\n",
        "\n",
        "      # Z-score corresponding to 1 - alpha/2\n",
        "      Z_alpha = stats.norm.ppf(1-alpha/2)\n",
        "\n",
        "      # mixing parameter\n",
        "      t = (Z_alpha**2) * ((np.var(x_t) + np.var(x_c)) / (len(x_c) + len(x_t)))\n",
        "\n",
        "      # e-term\n",
        "      eterm = (((len(x_c)+len(x_t))**2) * (t**2) * ((np.mean(x_t) - np.mean(x_c))**2)) / (2*v*(v+(len(x_t)+len(x_c))*(t**2)))\n",
        "\n",
        "      delta = np.sqrt(v/(v+(len(x_c)+len(x_t))*(t**2)))*np.exp(eterm)\n",
        "\n",
        "      # p-val\n",
        "      p_value = 1/delta\n",
        "\n",
        "      # Check if p-value is significant\n",
        "      if p_value < alpha:\n",
        "          return 1  # Significant result found, return 1\n",
        "\n",
        "  # No significant result found after testing all days\n",
        "  return 0\n"
      ],
      "metadata": {
        "id": "-5yCytBNSQmV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def msprt_ci(control, treatment, alpha = 0.05):\n",
        "  \"\"\"\n",
        "  performs msprt\n",
        "  \"\"\"\n",
        "  # Iterate over each day\n",
        "  for day in range(1, max(control['day']) + 1):\n",
        "      # Filter data up to the current day\n",
        "      x_c = control[control['day'] <= day]['action_count']\n",
        "      x_t = treatment[treatment['day'] <= day]['action_count']\n",
        "\n",
        "      # variance of the difference in means\n",
        "      v = (np.var(x_c) / len(x_c)) + (np.var(x_t) / len(x_t))\n",
        "\n",
        "      # Z-score corresponding to 1 - alpha/2\n",
        "      Z_alpha = stats.norm.ppf(1-alpha/2)\n",
        "\n",
        "      # mixing parameter\n",
        "      t = (Z_alpha**2) * ((np.var(x_t) + np.var(x_c)) / (len(x_c) + len(x_t)))\n",
        "\n",
        "      # marin of error term\n",
        "      me = np.sqrt(((v*(v+t))/t) * (-2*np.log(alpha/2) - np.log(v/(v+t))))\n",
        "\n",
        "      # sample mean\n",
        "      e = np.mean(x_t) - np.mean(x_c)\n",
        "\n",
        "      # ci\n",
        "      ci_l = e - me\n",
        "      ci_u = e + me\n",
        "\n",
        "      # Check if significant\n",
        "      if ci_u < 0 or ci_l > 0:\n",
        "          return 1  # Significant result found, return 1\n",
        "\n",
        "  # No significant result found after testing all days\n",
        "  return 0\n"
      ],
      "metadata": {
        "id": "1YEozIjDKsmf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulate A/A Tests"
      ],
      "metadata": {
        "id": "DzQaE82QPQw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a 1000 A/A tests each for each method:"
      ],
      "metadata": {
        "id": "CKnnXqlRPT4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixed Horizon Test"
      ],
      "metadata": {
        "id": "FGB8W1ppPZZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(1000):\n",
        "  control, treatment = test_data(relative_lift = 0)\n",
        "  results.append(fixed_horizon_test(control=control, treatment=treatment))\n",
        "\n",
        "print('FPR: ',np.mean(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX46agrND9Ws",
        "outputId": "e2bfe6bc-d941-46a5-d576-c14bc8abd297"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPR:  0.051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Peeking Test"
      ],
      "metadata": {
        "id": "xfyyeGjfPyqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(1000):\n",
        "  control, treatment = test_data(relative_lift = 0)\n",
        "  results.append(peeking_test(control=control, treatment=treatment))\n",
        "\n",
        "print('FPR: ',np.mean(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8jcz6sBMaHw",
        "outputId": "59e1431a-27c7-454b-c0ec-d38c61137337"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPR:  0.215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Test (mSPRT)"
      ],
      "metadata": {
        "id": "OAttRPEzBMPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(1000):\n",
        "  control, treatment = test_data(relative_lift = 0)\n",
        "  results.append(msprt_ci(control=control, treatment=treatment))\n",
        "\n",
        "print('FPR: ',np.mean(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KbSoYapP92b",
        "outputId": "5753e48d-ae24-40cb-a756-d72edd2f6516"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPR:  0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that these results are very similar to StatSig's results at ([Stewart, 2023](https://www.statsig.com/blog/sequential-testing-on-statsig))."
      ],
      "metadata": {
        "id": "Ua_OhhiejleD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Power"
      ],
      "metadata": {
        "id": "Dlfuq7uaP6Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we use van Belle's formula to get a sample size estimate for 80% power and 5% alpha:\n",
        "\n",
        "$$n = \\frac{16\\sigma^2}{\\delta^2}$$\n",
        "\n",
        "for each variant"
      ],
      "metadata": {
        "id": "H9qcmEJgBMR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_size_calculator(relative_lift=0.01):\n",
        "  control, treatment = test_data(relative_lift=relative_lift, traffic_per_day=100, days=14, param=0.037)\n",
        "\n",
        "  n = (16 * (np.var(control['action_count']))) / ((np.mean(control['action_count']*(relative_lift)))**2)\n",
        "\n",
        "  return round(n)"
      ],
      "metadata": {
        "id": "Sd-TS5l6RuB6"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size_calculator(relative_lift = 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP6BP1swRRD5",
        "outputId": "2156790f-0567-43e1-acd6-2e5121b2bc59"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37698"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need about 40,000 per variant. So we can do a 100 day test with 400 traffic per day."
      ],
      "metadata": {
        "id": "8xpWAYqwYeCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixed Horizon Test"
      ],
      "metadata": {
        "id": "4JRTwMszYstF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(100):\n",
        "  control, treatment = test_data(relative_lift = 0.1,days=100,traffic_per_day = 400)\n",
        "  results.append(fixed_horizon_test(control=control, treatment=treatment))\n",
        "\n",
        "print('Power: ',np.mean(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b97wp9O8RSDs",
        "outputId": "2c411050-3c9e-4412-d5b3-dfb966cb3552"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power:  0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(100):\n",
        "  control, treatment = test_data(relative_lift = 0.1,days=100, traffic_per_day = 400)\n",
        "  results.append(peeking_test(control=control, treatment=treatment))\n",
        "\n",
        "print('Power: ',np.mean(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8nK0yT4Y1yr",
        "outputId": "4f375f9b-67d0-4e64-c3bf-2fb796f075ce"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power:  0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(100):\n",
        "  control, treatment = test_data(relative_lift = 0.1,days=100, traffic_per_day = 400)\n",
        "  results.append(msprt_ci(control=control, treatment=treatment))\n",
        "\n",
        "print('Power: ',np.mean(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFvlu7i9Zd33",
        "outputId": "7daae639-3910-48e0-b1af-ce13bfbe7a80"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power:  0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much like in ([Stewart, 2023](https://www.statsig.com/blog/sequential-testing-on-statsig)), the Power is lower in mSPRT than it is with fixed horizon. This is the primary reason for why Sequential Testing is not told to be unanimously superior. If it were as superior, then sequential testing would always be recommended over fixed horizon."
      ],
      "metadata": {
        "id": "na4gxm1tj6Q_"
      }
    }
  ]
}
