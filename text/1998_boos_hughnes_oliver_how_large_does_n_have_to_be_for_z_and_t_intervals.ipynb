{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML4XH1Gn3fPeFlsokuGwc4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Boos and Hughnes-Oliver, 1998) How Large Does $n$ Have to Be for $Z$ and $t$ Intervals?\n",
        "\n",
        "[Paper Link](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=370418ed3d60901c7da87a095230463e54a16e00)\n",
        "\n",
        "## Abstract\n",
        "\n",
        "Reviews the role of $\\sqrt{\\beta_1}(X) / \\sqrt{n}$, where $\\sqrt{\\beta_1}(X)$ is the skewness coefficient of the random sample, in the answer to the question of the title of the paper.\n",
        "\n",
        "Also suggests a simple exercise for determining rules of thumb for $n$ that result in appropriate confidence interval coverage."
      ],
      "metadata": {
        "id": "zalcFNgEAcDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "In many courses, we represent the CLT and related \"Z\" intervals:\n",
        "\n",
        "$$\\left(\\bar X - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar X + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right)$$\n",
        "\n",
        "followed by the $t$ statistic and related \"t\" intervals:\n",
        "\n",
        "$$\\left(\\bar X - t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}, \\bar X + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right)$$\n",
        "\n",
        "These formulas assume a random sample $X_1,...,X_n$ with:\n",
        "- $E(X_i)=\\mu$\n",
        "- $\\text{var}(X_i)=\\sigma^2 < \\infty$\n",
        "- $\\bar X$ is the sample mean\n",
        "- $S^2$ is the sample variance\n",
        "- $z_{\\alpha/2}$ and $t_{\\alpha/2,n-1}$ are the $1-\\alpha/2$ quantiles of the standard normal and $t$ with $n-1$ degrees of freedom distributions, respectively.\n",
        "\n",
        "As instructors, we make the point that the intervals above have exact $1-\\alpha$ coverage for normally distributed data and approximate $1-\\alpha$ coverage for non-normal data, where the approximation improves with increasing $n$.\n",
        "\n",
        "Students invariably then ask, \"How large does $n$ have to be?\"\n",
        "\n",
        "It depends mostly on the skewness of the $X$ density (and to a lesser degree on kurtosis and other aspects of non-normality).\n",
        "\n",
        "In introductory courses it usually suffices to mention skewness and to give a few histograms of $Z_n=\\sqrt{n}(\\bar X-\\mu)/\\sigma$ and $t_n=\\sqrt{n}(\\bar X-\\mu)/S$ for, say, a symmetric density such as the uniform and a skewed density such as the exponential.\n",
        "\n",
        "The Pearson Skewness Coefficient\n",
        "\n",
        "$$\\sqrt{\\beta_1}=E\\{X-E(X)\\}^3 / \\{\n",
        "  \\text{var}(X)\\}^{3/2}$$\n",
        "\n",
        "offers additional information on the speed of convergence of $Z_n=\\sqrt{n}(\\bar X-\\mu)/\\sigma$. Since any normal distribution has $\\sqrt{\\beta_1}(X)=0$, we can compute the skewness of $Z_n$ and compare it to 0.\n",
        "\n",
        "In a derivation that we skip here, we obtain\n",
        "\n",
        "$$\\sqrt{\\beta_1}(Z_n)=\\sqrt{\\beta_1}(\\bar X)=\\sqrt{\\beta_1}(X)/\\sqrt{n}$$\n",
        "\n",
        "This paper explains:\n",
        "- the ubiquitous role of $\\sqrt{\\beta_1}(X)/\\sqrt{n}$ in the convergence of both $Z_n$ and $t_n$\n",
        "- explain the dramatic difference between $Z$ and $t$ confidence intervals for skewed data\n",
        "- rule of thumb for $n$ that result in appropriate confidence interval coverage\n",
        "- use regression for several common skewed distributions to obtain rules of thumb for how large $n$ needs to be.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xUT83HaHAzWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Skewness and the CLT\n",
        "\n",
        "The central limit theorem tells us that\n",
        "\n",
        "$$P(Z_n\\leq t)\\rightarrow P(Z\\leq t)\\text{ as }n\\rightarrow\\infty,\\text{ for all }t\\in (-\\infty,\\infty)$$\n",
        "\n",
        "where $Z$ is a standard normal random variable.\n",
        "\n",
        "The Pearson Skewness Coefficient is one possible way to quantify the speed of this convergence to normality. A heuristic argument for this is that $\\sqrt{\\beta_1}(Z_n)=\\sqrt{\\beta_1}(X)/\\sqrt{n}$ approaches $0$ (the value corresponding to the standard normal skewness coefficient) as either $n\\rightarrow\\infty$ or $\\sqrt{\\beta_1}(X)\\rightarrow 0$. Thus, in terms of the skewness coefficient, $Z_n$ inherits the direction of skewness of the $X$ distribution and converges to $Z$ at a rate $O(1/\\sqrt{n})$.\n",
        "\n",
        "> So what does the above mean? It's saying that $\\sqrt{\\beta_1}(X)/\\sqrt{n}$ can approach in two ways - one if the denominator approaches infinity, and the other is if the numerator approaches 0. If we focus on the numerator approach, we're saying that for every step that $\\sqrt{\\beta_1}(X)$ makes towards 0, $\\sqrt{\\beta_1}(Z_n)$ takes a step by $1/\\sqrt{n}$ because the relationship between the skewness coefficients of $Z_n$ and $X$ is such.\n",
        "\n",
        "For a more direct reasoning that the convergence (of $P(Z_n\\leq t)\\rightarrow P(Z\\leq t)$) depends on $\\sqrt{\\beta_1}(X)/\\sqrt{n}$, we consider the one-term Edgeworth expansion of $Z_n$:\n",
        "\n",
        "$$P(Z_n\\leq t)\\approx P(Z\\leq t)-\\frac{\\sqrt{\\beta_1}(X)(t^2-1)}{\\sqrt{n}\\cdot 6}\\phi(t)$$\n",
        "\n",
        "where $\\phi(t)$ is the standard normal density function.\n",
        "\n",
        "Note how the closer $\\sqrt{\\beta_1}(X)/\\sqrt{n}$ is to 0, the closer the distribution of $Z_n$ is to $Z$.\n",
        "\n",
        "Recall that the skewness coefficient of $Z_n$ which is $\\sqrt{\\beta_1}(X)/\\sqrt{n}$, \"inherits\" the skewness of the $X$'s (but diluted by $\\sqrt{n}$): if the $X$'s are positively skewed (right tail), then so is $Z_n$. And similarly for negative skew.\n",
        "\n",
        "We can see this in the Edgeworth expansion. Suppose $X$'s are positively skewed.\n",
        "\n",
        "For standard normal quantiles ($z_{\\alpha/2} > 1$, the edgeworth expansion gives $P(Z_n\\leq z_{\\alpha/2}) < 1-\\alpha/2$.\n",
        "\n",
        "> Why is it less than instead of greater than? Look at the Edgeworth expansion. We see how $P(Z\\leq t)$ is greater than $P(Z_n\\leq t)$, meaning that $1-\\alpha/2$ is greater than $P(Z_n\\leq z_{\\alpha/2})$\n",
        "\n",
        "This implies that the true $1-\\alpha/2$ quantile $\\xi_{1-\\alpha/2}$ of $Z_n$ is larger than $z_{\\alpha/2}$.\n",
        "\n",
        "> Why? To cause the inequality above to become equal, we have to raise the $z_{\\alpha/2}$ of $P(Z_n\\leq z_{\\alpha/2})$, thus it is larger.\n",
        "\n"
      ],
      "metadata": {
        "id": "kfYfnFPoAzY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. How Large Should $n$ Be?\n",
        "\n",
        "This section explains that the Edgeworth expansions are not accurate enough to determine $n$. They use a variety of Gamma distributions with varying parameters to show various levels of samples needed for what degree of skew. This is where it becomes clear where the \"over sample size of 30\" that is often touted in introductory courses in Statistics comes from.\n",
        "\n",
        "I didn't take notes here because I felt that for practical purposes it would be better to just follow the Kohavi paper on skew and coverage at [Kohavi et al., 2014](https://www.exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf). I explore these deeper in a simulation notebook which you can find in the simulations folder."
      ],
      "metadata": {
        "id": "jCMgKIeiAzbe"
      }
    }
  ]
}