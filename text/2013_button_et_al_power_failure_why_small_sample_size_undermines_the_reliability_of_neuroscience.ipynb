{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwyMgTp84rjkFLtq4Xl7eJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Button et al., 2013) Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience"
      ],
      "metadata": {
        "id": "f1LsZOPkhb4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/nrn3475"
      ],
      "metadata": {
        "id": "rLgqtpH4lt2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract"
      ],
      "metadata": {
        "id": "aFTwEXEIhb6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect.\n",
        "\n",
        "Here we show that the average statistical power of studies in neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results."
      ],
      "metadata": {
        "id": "lE5x-Byahb9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "oiwe1sKbhxky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practices that reduce the likelihood that the findings reflect a true (non-null) effect include using flexible study designs and flexible statistical analyses and running small studies with low statistical power.\n",
        "\n",
        "Low statistical power (because of low sample size, small effects, or both) negatively affects the likelihood that a nominally statistically significant finding actually reflects a true effect."
      ],
      "metadata": {
        "id": "J9Nx4yNchxnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low power in the absense of other biases"
      ],
      "metadata": {
        "id": "_m0roCcfhxpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 main problems contribute to producing unreliable findings in studies with low power, even when all other research practices are ideal\n",
        "1. low probability of finding true effects\n",
        "2. low positive predictive value when an effect is claimed\n",
        "3. exaggerated estimate of the magnitude of the effect when a true effect is discovered\n",
        "\n",
        "Note that a \"Positive Predictive Value\" is the probability that a 'positive' research finding reflects a true effect (that is, the finding is a true positive). This probability of a research finding reflecting a true effect depends on the prior probability of it being true (before doing the study), the statistical power of the study and the level of statistical significance.\n",
        "\n",
        "**First**, low power, by definition, means that the chance of discovering effects that are genuinly true is low. That is, low-powered studies produce more false negatives than high-powered studies. When studies in a given field are designed with a power of 20%, it means that if there are 100 genuine non-null effects to be discovered in that field, these studies are expected to only discover 20 of them.\n",
        "\n",
        "**Second**, the lower the power of the study, the lower the probability that an observed effect that passes the required threshold of claiming its discovery actually reflects the true effect. (that is, reaching nominal statistical significance, such as p < 0.05). In other words, the chance that getting a p value of under 0.05 actually reflects a non-null effect. This is called the PPV of a claimed discovery.\n",
        "\n",
        "$$\\text{PPV}=\\frac{(1-\\beta)\\times R}{(1-\\beta)\\times R+\\alpha}$$\n",
        "\n",
        "where\n",
        "- $1-\\beta$: power\n",
        "- $\\beta$: type 2 error (p(false negative))\n",
        "- $\\alpha$: type 1 error (p(false positive))\n",
        "- $R$: pre-study odds (the odds that a probed effect is indeed non-null among the effects being probed)\n",
        "\n",
        "For example, let's say:\n",
        "- we work in a field where 25% of effects we test are expected to be truly non-null ($R=0.25$).\n",
        "- we claim to have discovered an effect when we reach $p< 0.05$\n",
        "- assume our studies have $20\\%$ power\n",
        "\n",
        "Then, $\\text{PPV} = 0.5$, meaning that only half of our claims for our discoveries will be correct. If we replace the above with 80% power, then $\\text{PPV}=0.8$.\n",
        "\n",
        "**Third**, even when an underpowered study discovers a true effect, it is likely that the estimate of the magnitude of that effect provided by that study will be exaggerated. This effect inflation is often referred to as the 'winner's curse' and is likely to occur whenever claims of discovery are based on thresholds of statistical significance or other selection filters. Effect inflation is worst for small, low-powered studies studies, which can only detect effects that happen to be large.\n",
        "\n",
        "The winner's curse can also affect the design and conclusions of replication studies. If the original estimate of the effect is inflated, then replication studies will tend to show smaller effect sizes, as findings converge on the true effect."
      ],
      "metadata": {
        "id": "f9msEPWAhxrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low power in the presence of other biases"
      ],
      "metadata": {
        "id": "csRh5_tWnx2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low power is associated with several other biases. First, low-powered studies are more likely to provide a wide range of estimates of the magnitude of an effect. Second, publication bias, selective data analysis and selective reporting of outcomes are more likely to affect low-powered studies. Third, small studies may be of lower quality in other aspects of their design as well. These factors can further exacerbate the low reliability of evidence obtained in studies with low statistical power."
      ],
      "metadata": {
        "id": "ZQPuKkOgnx5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Empirical evidence from neuroscience"
      ],
      "metadata": {
        "id": "D32lFEX1nx9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any attempt to establish the average statistical power in neuroscience is hampered by the problem that the true effect sizes are not known.\n",
        "\n",
        "Meta-analysis provides the best estimate of the true effect size, albeit with limitations, including the limitation that the individual studies that contribute to a meta-analysis are themselves subject to the problems described above.\n",
        "\n",
        "Our results indicate that the median statistical power in neuroscience is 21%."
      ],
      "metadata": {
        "id": "zw0fE5j7CjQ3"
      }
    }
  ]
}