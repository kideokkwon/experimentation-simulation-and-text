{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUTL0OSsNzflDzGCSV9NH/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Imai, 2013) Statistical Hypothesis Tests"
      ],
      "metadata": {
        "id": "ylCAm3FH8YP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updates:\n",
        "- 12/3/23: wrote up to section 1.2\n",
        "- 12/4/23:"
      ],
      "metadata": {
        "id": "mCNwW9Jm8XqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a great number of introductory resources online on hypothesis testing. I've found Kosuke Imai's lecture notes on Statistical Hypothesis Tests for his class at Princeton to be one of the best out there, so we will take some notes on it below.\n",
        "\n",
        "The link is [here](https://imai.fas.harvard.edu/teaching/files/tests.pdf).\n",
        "\n",
        "For a brief background, Kosuke is a Professor (currently at Harvard) whose research focuses heavily involve causal inference. I have reviewed several of his papers as they have been very insightful. Some of my favorites are ([Ho et al., 2007](https://gking.harvard.edu/files/matchp.pdf)) and ([Imai and Kim, 2019](https://imai.fas.harvard.edu/research/files/FEmatch.pdf)).\n",
        "\n",
        "In addition, I add a lot of my own notes to help make it easier to understand (for my future self when I forget some of this stuff again!)"
      ],
      "metadata": {
        "id": "-bSgZcWO8dm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "G58mghv68dpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any statistical hypothesis test, no matter the complexity, is based on the logic of a *stochastic proof by contradiction*.\n",
        "\n",
        "One of the famous examples of a proof by contradiction involves proving that $\\sqrt{2}$ is an irrational number by contradiction. In a hypothesis test, it is *stochastic* because it involves uncertainty. This means that we can never conclude with certainty that a hypothesis is correct. Instead, we argue that the hypothesis is *likely* incorrect."
      ],
      "metadata": {
        "id": "VEZm8M3i8drh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Hypothesis Tests for Randomized Experiments"
      ],
      "metadata": {
        "id": "AzQgeFwN8dtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ronald Fisher invented the idea of statistical hypothesis testing."
      ],
      "metadata": {
        "id": "xmjy9hHk-4p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Lady Tasting Tea"
      ],
      "metadata": {
        "id": "2iRKPCQT-4rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In his book, Fisher (1935) illustrates the idea of hypothesis testing with a lady tasting tea who claimed she could tell if milk was poured first or if tea was poured first.\n",
        "\n",
        "This was tested by taking 8 cups, 4 of which the milk was poured in first and the rest where the tea was poured in first. The lady got every guess correct. How likely that this is pure luck?\n",
        "\n",
        "Fisher proposed to calculate the probability of observing the outcome that is *at least as extreme* as the outcome you actually observed under the null hypothesis as the *p-value*. Intuitively, if this probability is small, then you conclude that the null hypothesis is likely to be false.\n",
        "\n",
        "For Fisher's example, we can calculate this probability easily.\n",
        "\n",
        "Remember that there are $8!$ ways to arrange the $8$ cups, but because there are two groups where in each group we treat $4$ of them the same, we have to take out the duplicates which ends up with $\\frac{8!}{4!\\cdot 4!}$. A different way of thinking about this is how many different ways can you \"choose 4\" out of 8, which is simply ${8}\\choose{4}$. If she had gotten 6 correct, we would count the number of ways you can get 6 correct (or more extreme, which is 8). However, since she got all 8 right, that is just 1 event. Thus, the p-value is $1/70$.\n",
        "\n",
        "This procedure is referred to as *Fisher's exact test* because the test computes the *exact* p-value for testing the null hypothesis. It is also called a *permutation test* because it computes all permutations of treatment assignments.\n",
        "\n"
      ],
      "metadata": {
        "id": "z3_Mwu1P-4t_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Statistical Hypothesis Testing Procedure"
      ],
      "metadata": {
        "id": "ci0Yzu1N-4yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure can be summarized in 6 sequential steps:\n",
        "1. Choose $H_0$ and also $H_1$.\n",
        "2. Choose a threshold $\\alpha$, the maximum probability of *Type I error* one is willing to tolerate. Remember that Type I error is the probability of *falsely* rejecting the null. So thinking that there is an effect when there isn't.\n",
        "3. Choose a test statistic, which is a function of the observed data\n",
        "4. Derive a distribution of the test statistic under the null hypothesis. This distribution is often called the *reference distribution*.\n",
        "5. Compute the p-value by comparing the observed value of the test statistic against its reference distribution.\n",
        "6. Reject the null hypothesis if the p-value is less than the pre-specified threshold $\\alpha$ and retain the null hypothesis otherwise.\n",
        "\n",
        "Note that in Fisher's example, the null hypothesis is:\n",
        "$$H_0:Y_i(1)-Y_i(0)=0\\text{ for all }i$$\n",
        "$$H_1:Y_i(1)-Y_i(0)\\neq 0\\text{ for at least some }i$$\n",
        "\n",
        "This null is said to be *sharp* because the hypothesis is specified for each unit, which is a strong hypothesis because it assumes zero effect for *every* unit $i$.\n",
        "\n",
        "Since the example earlier of Fisher's exact test was limited to 8 data points, we can generalize it to any number of observations.\n",
        "\n",
        "Suppose we have a completely randomized experiment with binary treatment (canvassing votes). The total sample is $n$ units and randomly selected $n_1$ units are assigned to the treatment condition and the rest is assigned to the control group.\n",
        "\n",
        "The outcome is binary (turnout) and the test statistic is written as\n",
        "$$S=\\sum_{i=1}^{n}T_iY_i$$\n",
        "\n",
        "which amounts to just the # of users who had both $T_i=1$ and $Y_i=1$, so users in treatment who voted. So essentially we are seeing if canvassing votes lead to more votes.\n",
        "\n",
        "Under the sharp null hypothesis, same as above, Fisher shows that the distribution of the statistic is the *hyper-geometric distribution*,\n",
        "$$Pr(S=s)=\\frac{{m\\choose s}{n-m\\choose n_1-s}}{{n\\choose n_1}}$$\n",
        "\n",
        "where $m=\\sum_{i=1}^{n}Y_i$.\n",
        "\n",
        "For clarity,\n",
        "- $n$: total # of users\n",
        "- $m$: total # of users who voted ($Y_i=1$)\n",
        "- $n_1$: the # of users assigned to treatment\n",
        "- $s$: # of users assigned to treatment ($T_i=1$) who got outcome ($Y_i=1$)\n",
        "\n",
        "Therefore,\n",
        "\n",
        "The distribution above can be interpreted as:\n",
        "- $m\\choose s$: # of ways to get $s$ amount of users who got treatment out of total $m$ users who had $Y_i=1$\n",
        "- $n-m\\choose n_1-s$: # of ways to get users who got treatment ($T_i=1$) *and* outcome $Y_i=0$ out of total # of users ($n-m$) who got $Y_i=0$.\n",
        "- $n\\choose n_1$: # of ways to choose $n_1$ treatment users out of total $n$ users.\n",
        "\n",
        "Remember that binomial distribution can be thought of as \"probability of $s$ successes in $n_1$ draws with replacement\". On the other hand, hyper-geometric can be thought of as, \"probability of $s$ successes in $n_1$ draws *without* replacement\".\n",
        "\n",
        "So you can think of the above formula as the probability of getting an $s$ number of users ($T_i=1,Y_i=1$) out of total $n_1$ users ($T_i=1$) without replacement.\n",
        "\n",
        "Also note that this distribution can be approximated by a $\\text{Binomial}(n_1,\\frac{m}{n})$ when $n$ is large and $n_1/n$ is small.\n",
        "\n",
        "To make it more clear, imagine if we had:\n",
        "- $n=8$: 8 total users\n",
        "- $n_1=4$: 4 users who are went to a voting station with canvassing, thus 4 users without canvassing\n",
        "- $m=4$: 4 users who voted\n",
        "\n",
        "If I got $s=4$, as in all 4 people who had canvas voted, this means that all 4 people who didn't get a canvas didn't vote, and the chance of this happening is $1/70$, which you can get by plugging in the above in $HG(n_1=4,n=8,m=4)$ using any [calculator](https://homepage.divms.uiowa.edu/~mbognar/applets/hg.html).\n",
        "\n",
        "Of-course, the larger the $n$, the more permutations, and makes it difficult to compute the exact distribution. However, we can analytically obtain the following *exact* mean and variance:\n",
        "$$E(S)=\\frac{n_1\\cdot m}{n}, \\text{Var}(S)=\\frac{mn_1(n-n_1)}{n(n-1)}\\left(1-\\frac{m}{n}\\right)$$\n",
        "\n",
        "Thus, these moments can be then used to obtain an asymptotic approximation for a large $n$ via the central limit theorem:\n",
        "$$\\frac{\\{S-E(S)\\}}{\\sqrt{\\text{Var}(S)}}\\xrightarrow{d}\\mathcal{N}(0,1)$$\n",
        "\n",
        "Now what does the above math mean? It shows a few things - first off, it demonstrates the Central Limit theorem that show that the distribution of the sum of a large # of independent, identically distributed random variables approaches a normal distribution. In addition, it exploits the fact that we have the analytical formulas for the mean and the variance to standardize the normal distribution to the standard normal distribution.\n",
        "\n",
        "In a lot of statistical packages, the exact calculation is done for small sample sizes while the simulation method is used for large samples (e.g., `fisher.test()` in R). Fisher's exact test was later generalized by McNemar (1947) and Mantel and Haenszel (1959) to matched-pair and stratified designs, respectively."
      ],
      "metadata": {
        "id": "e9t_trxB-40i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Testing the Population Average Treatment Effect"
      ],
      "metadata": {
        "id": "IugmMwkR7g5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "57nAQkZa7g71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_sWKszfO7g-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y1pkTQ4b7hAS"
      }
    }
  ]
}