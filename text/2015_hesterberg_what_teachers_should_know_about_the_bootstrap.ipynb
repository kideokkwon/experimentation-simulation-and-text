{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtKj7XIWug8HVaSN/EVHrH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Hesterberg, 2015) What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum\n",
        "\n",
        "[Link](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2015.1089789)"
      ],
      "metadata": {
        "id": "Yovo0QxPUVUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "\n",
        "Bootstrapping has enormous potential but there are subtle issues and ways to go wrong. Goal of this article is to provide a deeper understanding of bootstrap methods - how they work, when they work or not, and which methods work better - and to highlight pedagogical issues."
      ],
      "metadata": {
        "id": "xqG16oFSUVWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "The bootstrap is used for estimating standard errors and bias, obtaining confidence intervals, and sometimes for tests.\n",
        "\n",
        "## 1.1 Verizon Example\n",
        "\n",
        "## 1.2 One-Sample Bootstrap\n",
        "\n",
        "Let $\\hat\\theta$ be a statistic calculated from a sample of $n$ iid observations (i.e., time series and other dependent data are beyond the scope of this article).\n",
        "\n",
        "In the ordinary *nonparametric bootstrap*, we draw $n$ observations *with replacement* from the original data to create a *bootstrap sample* or *resample*, and calculate the statistic $\\hat\\theta^{*} $ for this sample (use ${*}$ to denote a bootstrap quantity). The bootstrap statistics comprise the *bootstrap distribution*.\n",
        "\n",
        "Let's say we do this for two distributions (from the Verizon example) to estimate certain things about the corresponding sampling distribution, including:\n",
        "\n",
        "**standard error**: the *bootstrap standard error* is the sample standard deviation of the bootstrap distribution,\n",
        "\n",
        "$$s_b=\\sqrt{\\frac{1}{(r-1)}\\sum_{i=1}^{r}(\\hat\\theta_i^{*}-\\bar{\\hat\\theta}^{*})^2}$$\n",
        "\n",
        "**confidence intervals**: a quick-and-dirty interval, the *bootstrap percentile interval*, is the range of the middle 95% of the bootstrap distribution\n",
        "\n",
        "**bias**: the bootstrap bias estimate is $\\bar{\\hat\\theta}^{*}-\\hat\\theta$\n",
        "\n",
        "The bootstrap separates the concept of a standard error - the standard deviation of the sampling distribution - from the common formula $s / \\sqrt{n}$ for estimating the standard error of a sample mean.\n",
        "\n",
        "## 1.3 Two-Sample Bootstrap\n",
        "\n",
        "For a two-sample bootstrap, we independently draw bootstrap samples with replacement from each sample, and compute a statistic that compares the samples (compute the difference in means $\\bar x_1 - \\bar x_2$). The bootstrap distribution is centered at the observed statistic; it is used for confidence intervals and standard errors.\n",
        "\n",
        "## 1.4 Bootstrap $t$-distribution\n",
        "\n",
        "It is not surprising that $t$ procedures are inaccurate for skewed data with a sample of size 23, or for the difference when one sample is that small.\n",
        "\n",
        "More surprising is how bad $t$ confidence intervals are for the larger sample size of 1664. To see this, we bootstrap $t$ statistics.\n",
        "\n",
        "Above, we resampled *univariate* distributions of *estimators* like $\\bar x$ or $\\bar x_1 - \\bar x_2$. Here we look at joint distribution such as the joint of $\\bar X$ and $s$, and distributions of statistics that depend on both $\\hat\\theta$ and $\\theta$.\n",
        "\n",
        "To estimate the sampling distribution of $\\hat\\theta-\\theta$ ($\\hat\\theta$ is the sample statistic and $\\theta$ is the true population value), we use the bootstrap distribution of $\\hat\\theta^{*}-\\hat\\theta$. The boostrap bias estimate is $E(\\hat\\theta^{*}-\\hat\\theta)$, an estimate of $E(\\hat\\theta-\\theta)$. Thus, to estimate the sampling distribution of a $t$-statistic\n",
        "\n",
        "$$t=\\frac{\\hat\\theta-\\theta}{\\text{SE}}$$\n",
        "\n",
        "where $\\text{SE}$ is a standard error calculated from the original sample, we use the bootstrap distribution of\n",
        "\n",
        "$$t^{*}=\\frac{\\hat\\theta^{*}-\\hat\\theta}{\\text{SE}^{*}}$$\n",
        "\n",
        "The amount of skewness apparent in the bootstrap t-distribution matters. The bootstrap distribution is a sampling distribution, not raw data; the CLT has already had its one chance to work. At this point, any deviations indicate errors in procedures that assume normal or t sampling distributions.\n",
        "\n",
        "A common flaw in statistical practice is to fail to judge how accurate standard CLT-based methods are for specific data; the bootstrap t-distribution provides an effective way to do so.\n",
        "\n",
        "> To summarize, you can use the Bootstrap to estimate the sample distribution of a *statistic*. For example, when you are performing a t-test, you are likely assuming via the CLT that t-statistic has sufficiently converged to normal. since the statistic involves $\\bar X$, you only see that one point and not the distribution of the statistic. The bootstrap allows you to do so.\n",
        "\n",
        "## 1.5 Pedagogical and Practical Value\n",
        "\n",
        "Students can obtain confidence intervals by working directly with the statistic of interest, rather than using a t-statistic. In mathematical statistics, students can use the bootstrap to help understand joint distributions of estimators like $\\bar X$ and $s$, and to understand the distribution of $t$ statistics, and compute bootstrap t confidence intervals.\n",
        "\n",
        "Resampling is also important in practice. It often provides the only practical way to do inference - when it is too difficult to derive formulas, or the data stored in a way that make calculating the formulas impractical.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i1Bv1kbLUVYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. The Idea Behind Bootstrapping\n",
        "\n",
        "Inferential statistics is based on sampling distributions. In theory, to get these we draw (all or infinitely many) samples from the *population*, and compute teh statistic of interest for each sample (such as the mean,  median, etc.,)\n",
        "\n",
        "The distribution of the statistics is the *sampling distribution*.\n",
        "\n",
        "In practice we cannot draw arbitrarily many samples from the population; we have only one sample. The bootstrap idea is to draw samples from an estimate of the population, in lieu of the population:\n",
        "\n",
        "draw samples from *an estimate* of the population, and compute the statistic of interest for each sample. The distribution of the statistics is the *bootstrap distribution*.\n",
        "\n",
        "## 2.1 Plug-In Principle\n",
        "\n",
        "The bootstrap is based on the *plug-in principle* - if something is unknown, we substitute an estimate for it. For example, the sd of the sample mean is $\\sigma /\\sqrt{n}$; when $\\sigma$ is unknown we substitute an estimate for $s$, the sample standard deviation.\n",
        "\n",
        "With the bootstrap we go one step farther - instead of plugging in an estimate for a single parameter, we plug in an estimate for the whole populatoin $F$.\n",
        "\n",
        "So what do we substitute for $F$? This includes nonparametric, parametric, and smoothed bootstrap. The primary focus of this article is the nonparametric bootstrap (most common), which consists of drawing samples from the empirical distribution $\\hat F_n$ (with probability $1/n$ on each observation), that is, drawing samples with replacement from the data.\n",
        "\n",
        "In the parametric bootstrap, we assume a model (e.g., a gamma with unknown shape and scale), estimate parameters for that model, then draw bootstrap samples from the model with those estimated parameters.\n",
        "\n",
        "The smoothed bootstrap is a compromise between para and nonpara. If we believe the population is continuous, we may sample from a continuous $\\hat F$, say a kernel density estimate. Smoothing is not common; it is rarely needed, and does not generalize well to multivariate and factor data.\n",
        "\n",
        "## 2.2 Fundamental Bootstrap Principle\n",
        "\n",
        "The fundamental Bootstrap principle is that this substitution *usually works* - that we can plug in an estimate for $F$, then sample, and the resulting bootstrap distribution provides useful information about the sampling distribution.\n",
        "\n",
        "## 2.3 Inference, Not Better Estimates\n",
        "\n",
        "*The bootstrap distribution is centered at the observed statistic, not the population parameter*, for example, at $\\bar x$, not $\\mu$.\n",
        "\n",
        "Two profound implications.\n",
        "\n",
        "1. We do not use the mean of the bootstrap statistics as a replacement for the original estimate - we cannot use the bootstrap to improve on $\\bar x$. Instead, we use the bootstrap to tell how accurate the original estimate is.\n",
        "\n",
        "In this regard, the bootstrap is like formula methods that use the data twice - once to compute an estimate, and again to compute a standard error for the estimate. The bootstrap just uses a different approach to estimating the standard error.\n",
        "\n",
        "2. We do not use the CDF or quantiles of the bootstrap distribution of $\\hat\\theta^{*}$ to estimate the CDF or quantiles of the sampling distribution of an estimator $\\hat\\theta$. Instead, we bootstrap or estimate things like the standard deviation, the expected value of $\\hat\\theta - \\theta$, and the CDF and quantiles of $\\hat\\theta-\\theta$ or $(\\hat\\theta-\\theta)/\\text{SE}$.\n",
        "\n",
        "## 2.4 Key Idea Versus Implementation Details\n",
        "\n",
        "if $n$ is small we could evaluate all possible bootstrap samples. This is called an *exhaustive boostrap* or *theoretical bootstrap*. Since exhaustive methods are infeasible, we draw 10,000 random samples instead; we call this the *Monte Carlo sampling implementation*.\n",
        "\n",
        "## 2.5 How to Sample\n",
        "\n",
        "We typically sample with the same size as the original data - because by doing so the standard errors reflect the actual data, rather than a hypothetical larger or smaller dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "BYNrF9k2Uy7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Variation in Bootstrap Distributions\n",
        "\n",
        "We claimed above that the bootstrap distribution usually provides useful information about the sampling distribution. We elaborate on that here and address two questions:\n",
        "1. How accurate is the theoretical (exhaustive) bootstrap?\n",
        "2. How accurately does the Monte Carlo implementation approximate the theoretical bootstrap?\n",
        "\n",
        "## 3.1 Sample Mean: Large Sample Size\n",
        "\n",
        "Remember, the bootstrap:\n",
        "- does not provide a better estimate of the population parameter\n",
        "- similarly, quantiles of the bootstrap distributions are *not* useful for estimating quantiles of the sampling distribution\n",
        "- instead, the bootstrap distributions are useful for estimating the *spread* and *shape* of the sampling distribution.\n",
        "\n",
        "We also see examples with r = 1000 vs r = 10^4 resamples. Using more samples reduces random Monte Carlo variation, but does not fundamentally change the bootstrap variation - still same approximate center, spread, and shape.\n",
        "\n",
        "The variation that comes from the Monte Carlo is much smaller than the variation due to different original samples, so for many uses such as a quick-and-dirty estimation of standard errors for the test statistic, r=1000 resamples is adequate.\n",
        "\n",
        "## 3.2 Sample Mean: Small Sample Size\n",
        "\n",
        "The bootstrap distributions tend to be too narrow on average, by a factor of $\\sqrt{(n-1)/n}$ for the sample mean and approximately that for many other statistics.\n",
        "\n",
        "\n",
        "This goes back to the plug-in principle; the empirical distribution has variance $\\hat\\sigma^2=\\text{var}_{\\hat F_n}(X)=\\frac{1}{n}\\sum (x_i-\\bar x)^2$, and the theoretical bootstrap standard error is the standard deviation of a mean $n$ independent observations from that distributions, $s_b=\\hat\\sigma / \\sqrt{n}$. That is, smaller than the usual formula $s/\\sqrt{n}$ by a factor of $\\sqrt{(n-1)/n}$.\n",
        "\n",
        "This *narrowness bias* and variability in spread makes some bootstrap CI's under-cover.\n",
        "\n",
        "## 3.3 Sample Median\n",
        "\n",
        "The ordinary bootstrap tends not to work well for statistics such as the median or other quantiles in small samples that depend heavily on a small number of obesrvations out of a larger sample.\n",
        "\n",
        "## 3.4 Mean-Variance Relationship\n",
        "\n",
        "In many applications, the sperad or shape of the sampling distribution depends on the parameter of interest. For example, for an exponential distribution, the standard deviation of the sampling distribution of $\\bar x$ is proportional to $\\mu$.\n",
        "\n",
        "## 3.5 Summary of Visual Lessons\n",
        "\n",
        "The bootstrap distribution reflects the original *sample*. If the sample is narrower than the population, the bootstrap distribution is narrower than the samplinlg distribution.\n",
        "\n",
        "Typically for large samples the data represent the population well.\n",
        "\n",
        "*Bootstrapping does not overcome the weakness of small samples as a basis for inference*. For very small samples, it may be better to make additional assumptions such as a parametric family.\n",
        "\n",
        "Looking ahead, two things matter for accurate inferences:\n",
        "1. How close the bootstrap distribution is to the sampling distribution\n",
        "2. How well the procedures allow for variation in samples (e.g., fudge factor).\n",
        "\n",
        "## 3.6 How Many Bootstrap Samples\n",
        "\n",
        "A bootstrap distribution based on $r$ random samples corresponds to drawing $r$ observations with replacement from the theoretical bootstrap distribution.\n",
        "\n",
        "We can quantify the Monte Carlo variation in two ways - using formulas, or by bootstrapping.\n",
        "\n",
        "Let $G$ be the CDF of a theoretical bootstrap distribution and $\\hat G$ the Monte Carlo approximation, then the variance of $\\hat G(x)$ is $G(x)(1-G(x))/r$, which we can estimate using\n",
        "\n",
        "$$\\hat G(x)(1-\\hat G(x))/r$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1fMPFb6_Uy99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Confidence Intervals\n",
        "\n",
        "This section describes a number of confidence intervals and compare their pedagogical value and accuracy.\n",
        "\n",
        "## 4.1 Statistics 101 - Percentile, and $t$ with Bootstrap SE\n",
        "\n",
        "Neither the bootstrap percentile interval or the t-interval is very accurate. They are only first-order accurate, and poor in small samples - they tend to be too narrow. The bootstrap SE is too small by a factor of $\\sqrt{(n-1)/n}$, thus the $t$ interval with bootstrap SE is too narrow by that factor.\n",
        "\n",
        "The percentile interval suffers the same narrowness and more.\n",
        "\n",
        "In practice, the $t$ with bootstrap standard error offers no advantage over a standard $t$ procedure for the sample mean. Its advantages are pedagogical, and that it can be used for statistics that lack easy standard error formulas.\n",
        "\n",
        "The percentile interval is not a good alternative to standard $t$ intervals for the mean of small samples - while it handles skewed populations better, it is less accurate for small samples because it is too narrow. For exponential populations, it is less accurate than standard $t$ for $n\\leq 34$.\n",
        "\n",
        "## 4.2 Reverse Bootstrap Percentile Interval\n",
        "\n",
        "## 4.3 Bootstrap $t$ Interval\n",
        "\n",
        "The bootstrap $t$ confidence interval is based on the $t$ statistic, but estimates quantiles of the actual distribution using the data rather than a table.\n",
        "\n",
        "## 4.4 Confidence Interval Accuracy\n",
        "\n",
        "For a 95% interval, a perfectly accurate interval misses the parameter 2.5% of the time on each side.\n",
        "\n",
        "## 4.5 Skewness and Mean-Variance Relationship\n",
        "\n",
        "## 4.6 Confidence Interval Details\n",
        "\n",
        "## 4.7 Bootstrap Hypothesis Testing\n",
        "\n",
        "Two borad approaches.\n",
        "\n",
        "1. Invert a CI - reject $H_0$ if the corresponding interval excludes $\\theta_0$.\n",
        "\n",
        "2. Sample in a way that is consistent with $H_0$, then calculate a $p$-value as a tail probability. However this is not as accurate as a permutation test.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SDI57J-t7fz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Regression\n",
        "\n",
        "Two ways that bootstrapping in regression is particularly useful pedagogically.\n",
        "\n",
        "1. Understand the variability of regression predictions by a graphical bootstrap.\n",
        "\n",
        "A bootstrap percentile CI for $E(Y|x)$ is the range of the middle 95% of the $y$ values for regression lines at any $x$; these intervals are wider for more extreme $x$.\n",
        "\n",
        "2. Understand the difference between confidence and prediction intervals.\n",
        "\n",
        "The bootstrap esetimates the performance of the model that was actually fit to the data, regardless of whether that is a poor model.\n",
        "\n",
        "## 5.1 Resample Observations or Conditional Distributions\n",
        "\n",
        "Two common procedures when bootstrapping regression are\n",
        "\n",
        "1. bootstrap observations\n",
        "2. bootstrap residuals\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMaoAmhK7f2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K8WlqY8YDbw4"
      }
    }
  ]
}