{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIeucXqaB+BNcyNshFxVCB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Ting and Hung, 2023) On the Limits of Regression Adjustment"
      ],
      "metadata": {
        "id": "kGFDsbOpRgcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2311.17858.pdf"
      ],
      "metadata": {
        "id": "ilxjhKHnRgem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ],
      "metadata": {
        "id": "1NWk5k-7RghG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression adjustment, also known as Controlled-experiment Using Pre-Experiment Data (CUPED) is an important technique in internet experimentation. It decreases the variance of effect size estimates, often cutting confidence interval widths in half or more while never making them worse. It does so by carefully regressing the goal metric against pre-experiment features to reduce the variance.\n",
        "\n",
        "The tremendous gains of regression adjustment begs the question: How much can we do by engineering better features from pre-experiment data, for example by using machine learning techniques or synthetic controls? Could we even reduce the variance in our effect sizes arbitrarily close to zero with the right predictors?\n",
        "\n",
        "Unfortunately, the answer is negative. A simple form of regression adjustment which uses just the pre-experiment values of the goal metric captures most of the benefit. Specifically, under a mild assumption that observations closer in time are easier to predict than ones further away in time, we upper bound the potential gains of more sophisticated feature engineering, with respect to the gains of this simple form of regression adjustment. The maximum reduction in variance is 50%, or equivalently, the confidence interval width can be reduced by at most an additional 29%\n",
        "\n",
        "This result allows us to assess the potential value of investing in more advanced versions of regression adjustment. Furthermore, this upper bound suggests a fundamental boundary for how much analysis-based variance reduction that adjusts random imbalance from sampling can achieve. Breaking this barrier may require injecting domain nowledge on the causal mechanism which enables us to adjust for appropriate post-treatment variables or changing the experimental design directly."
      ],
      "metadata": {
        "id": "F7tOoHUdRgjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Main Result"
      ],
      "metadata": {
        "id": "0zt4o0fYRn0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $\\rho\\in[0,1]$ is the correlation between the post-experimental values and the pre-experiment values in the goal metric, the regression adjustment estimator reduces the variance by a factor of $1-\\rho^2$ (when $\\rho\\approx 1$ and the post-experiment values can be nearly perfectly predicted by the pre-experiment values, then the variance of the treatment effect estimator goes to $0$).\n",
        "\n",
        "While any covariates that are known to be unaffected by the treatment assignment is valid, pracitioners often restrict themselves to \"safe\" covariates that are pre-treatment.\n",
        "\n",
        "Under a reasonable assumption that the correlation between $X$ and $Y_\\text{post}$ is at most the correlation between $X$ and $Y_\\text{pre}$ (the implication here again is that it's easier to predict outcomes closer than farther away), we show that this \"safe\" form of advanced regression adjustment is limited in further variance reduction. Specifically,\n",
        "$$\\text{var}(\\hat\\delta_{\\text{advanced r.a}})\\geq\\frac{1}{1+\\rho}\\text{var}(\\hat\\delta_{\\text{basic r.a}})$$\n",
        "\n",
        "In particular, when basic regression adjustment is very effective ($\\rho\\approx 1$), advanced regression adjustment via feature engineering can only reduce it further by 50%; when basic regression adjustment is not effective ($\\rho\\approx 0$), advanced regression adjustment will not be effective either."
      ],
      "metadata": {
        "id": "5VUbFPN3Rn3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mathematical setup"
      ],
      "metadata": {
        "id": "HAFnf4WpRn5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose $Y_{\\text{pre}}$ and $Y_{\\text{post}}$ are the pre-experiment and post-experiment values in the goal metric of a randomly sampled user. Then the basic regression adjustment reduces the variance of the estimated average treatment effect by\n",
        "\n",
        "$$\\frac{\\text{var}(\\hat\\delta_{\\text{basic r.a}})}{\\text{var}(\\hat\\delta_{\\text{original}})}=1-\\rho^2=1-\\text{cor}(Y_{\\text{pre}},Y_{\\text{post}})^2$$\n",
        "\n",
        "More advanced regression adjustment methods attempt to design a covariate $X$ that is as correlated with $Y_\\text{post}}$ as possible. Typical advanced regression adjustment methods include:\n",
        "- Using multiple pre-treatment covariates\n",
        "- Producing predictions of $Y_{\\text{post}}$ based on external datasets.\n",
        "- Producing predictions of $Y_{\\text{post}}$ using $Y_{\\text{{post}}$. While not normally allowed, can use cross-fitting to avoid overfitting."
      ],
      "metadata": {
        "id": "oDB8N7SxRn76"
      }
    }
  ]
}